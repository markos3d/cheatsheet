# LLM Parametri - Cheatsheet ğŸ“š

Kratak vodiÄ kroz kljuÄne parametre za rad sa velikim jeziÄkim modelima (LLM).

## ğŸ›ï¸ KljuÄni parametri generacije

| Parametar | ObjaÅ¡njenje | Primer vrednosti |
|-----------|-------------|------------------|
| **Temperature (Temperatura)** | KontroliÅ¡e nasumiÄnost u izlazu. NiÅ¾a vrednost = konzervativnije i predvidljivije. ViÅ¡a vrednost = kreativnije i raznovrsnije. | `0` (precizno), `0.7` (balansirano), `1` (kreativno) |
| **Top-k** | Bira sledeÄ‡i token samo iz prvih *k* najverojatnijih kandidata. | `k=40` (ÄeÅ¡Ä‡e koriÅ¡Ä‡eno), `k=0` (iskljuÄeno) |
| **Top-p (Nucleus Sampling)** | Bira tokene dok zbirna verovatnoÄ‡a ne preÄ‘e prag *p*. DinamiÄki prilagoÄ‘ava broj izbora. | `p=0.9` (balansirano), `p=0.95` (kreativnije) |
| **Min-p** | Minimalna zbirna verovatnoÄ‡a pri izboru tokena (ograniÄava preveliku raznovrsnost). | `0.05â€“0.2` |
| **Repeat Penalty** | Smanjuje verovatnoÄ‡u da model ponavlja iste reÄi/fraze. | `1.0` (nema kazne), `1.1â€“1.2` (blaga), `1.3+` (jaka kazna) |
| **Max Tokens / Max Length** | OgraniÄava maksimalan broj generisanih tokena (duÅ¾ina odgovora). | `512`, `2048`, `4096` |

## ğŸ”§ Parametri modela

| Koncept | ObjaÅ¡njenje |
|---------|-------------|
| **Parameter Size (Broj parametara)** | VeÄ‡i modeli (npr. 13B, 70B) imaju bolje razumevanje, ali troÅ¡e viÅ¡e RAM-a/VRAM-a i sporiji su. Manji modeli (3B, 7B) su brÅ¾i i lakÅ¡i za lokalno pokretanje. |
| **Context Length (Kontekst / Context Window)** | Broj tokena koji model moÅ¾e "zapamtiti" u jednoj sesiji. VeÄ‡i prozor = model se bolje seÄ‡a dugih razgovora. |
| **Quantization (Kvantizacija)** | Tehnika smanjivanja veliÄine modela (npr. sa FP16 na INT8/INT4) radi manjeg zauzeÄ‡a RAM/VRAM. Neznatno smanjuje preciznost, ali omoguÄ‡ava pokretanje na slabijem hardveru. |
| **Precision** | Koliko bita se koristi za predstavljanje teÅ¾ina modela: FP16 (najpreciznije, ali ogromno), INT8 (balans), INT4 (najmanje zauzeÄ‡e, ali slabija taÄnost). |
| **GGUF / GPTQ / AWQ** | Formati kvantizovanih modela. GGUF je standard za Ollama i LM Studio, GPTQ je Äest na GPU setup-ima. |

## ğŸ’¡ Brzi saveti

- **Za kreativno pisanje**: Temperature `0.8-1.0`, Top-p `0.95`
- **Za precizne odgovore**: Temperature `0.1-0.3`, Top-k `20-40`
- **Za lokalno pokretanje**: Koristiti kvantizovane modele (GGUF format)
- **DugaÄke konverzacije**: VeÄ‡i context length (8k+ tokena)

---
*Kreiran kao referentni vodiÄ za rad sa LLM parametrima*